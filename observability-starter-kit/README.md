# Observability Starter Kit (Blueprint 09)

**VersÃ£o:** v1.0 â€¢ **Data:** 2026-01-03 â€¢ **Status:** P0 CanÃ´nico

Starter kit completo para observabilidade server-blind, alinhado Ã  ConstituiÃ§Ã£o, ErrorToken e JSONâœ¯Atomic.

---

## ğŸ“‹ Estrutura

```
observability-starter-kit/
  prometheus/
    prometheus.yml          # Scrape config + alerting rules
    alerts.yml              # Multi-burn rate SLO alerts
  otel-collector/
    config.yaml             # OTLP/HTTP â†’ Prometheus
  grafana/
    dashboards/
      00-executive.json     # Executive â€” LatÃªncia & Erros
      10-office-mcp.json    # Office â€” MCP Tooling
      20-gateway.json       # Gateway â€” LatÃªncia & Access
      30-core-api.json      # Core API â€” DB & Throughput
    provisioning/
      datasources/         # Auto-provision Prometheus
      dashboards/          # Auto-load dashboards
  README.md
```

---

## ğŸš€ Quick Start

### 1) OpenTelemetry Collector

O Collector recebe mÃ©tricas OTLP/HTTP do Worker e expÃµe `/metrics` no formato Prometheus.

```bash
# Start collector
otelcol --config ./otel-collector/config.yaml

# Listens on:
# - :4318 (OTLP/HTTP) â€” Worker envia aqui
# - :9464/metrics (Prometheus) â€” Prometheus scrape aqui
```

**Worker (Cloudflare):** Configure OTLP endpoint:
```typescript
const otlpEndpoint = 'http://<collector-host>:4318/v1/metrics';
```

**Rust services (Core/Office/Policy-Proxy):** Continuam expondo `/metrics` nativo.

---

### 2) Prometheus

Edite `prometheus/prometheus.yml`:
- Substitua `lab512.local` / `lab256.local` pelos seus hosts reais.
- Confirme que o target `otel-collector` estÃ¡ acessÃ­vel em `:9464`.

```bash
prometheus --config.file=./prometheus/prometheus.yml
```

**Jobs configurados:**
- `otel-collector` (Worker metrics via OTLP)
- `gateway` (Core API /metrics)
- `office` (Office /metrics)
- `policy-proxy` (Policy-Proxy /metrics)

**Alertas:** Carregados de `alerts.yml` (multi-burn rate SLO).

---

### 3) Grafana

**Import dashboards:**
1. Acesse Grafana UI
2. Import os 4 JSONs de `grafana/dashboards/`:
   - **00-executive.json** â€” Executive (LatÃªncia & Erros)
   - **10-office-mcp.json** â€” Office/MCP
   - **20-gateway.json** â€” Gateway (LatÃªncia & Access)
   - **30-core-api.json** â€” Core API (DB & Throughput)

**Auto-provisioning (opcional):**
- Configure `grafana/provisioning/datasources/prometheus.yml`
- Configure `grafana/provisioning/dashboards/dashboards.yml`

---

## ğŸ“Š MÃ©tricas Esperadas

Os dashboards assumem estas mÃ©tricas (ajuste PromQL se necessÃ¡rio):

### Gateway
- `gateway_http_requests_total{route,method,code}`
- `gateway_http_request_duration_seconds_bucket{route}`
- `gateway_backpressure_count`
- `webhook_delivery_total{dest,ok}`
- `webhook_delivery_duration_seconds_bucket{dest}`

### Office/MCP
- `office_mcp_call_total{tool,ok,err}`
- `office_mcp_call_duration_seconds_bucket{tool}`
- `office_ws_reconnect_ms_bucket`

### Core API
- `core_db_query_seconds_bucket{op}`
- `core_rate_limit_hits_total{bucket}`
- `core_http_requests_total{tenant}`
- `media_presign_total{ok}`

### Policy-Proxy
- `policy_eval_total{decision,reason}`
- `jwks_refresh_failure_total`

---

## ğŸš¨ Alertas (SLO Multi-Burn Rate)

**Gateway:**
- LatÃªncia p99 > 300ms (5m & 30m windows)
- Erro 5xx > 1% (5m) OU > 0.3% (1h)

**Office:**
- BACKPRESSURE > 2% (15m)
- WS reconnect p95 > 500ms
- MCP tool/call p99 > 300ms

**Core API:**
- DB query p99 > 500ms
- Rate limit hits > 10/min

**Policy-Proxy:**
- Policy deny rate > 10%
- JWKS refresh failures >= 3 (5m)

---

## ğŸ“ Logs (JSONL Server-Blind)

**Campos permitidos (lista fechada):**
```
ts, component, tenant, route, method, tool, session_id, correlation_id,
ok, err_token, code, latency_ms, bytes_in, bytes_out, cost_calls, node, trace_id
```

**Proibido:** `params`, `args`, `payload`, `plaintext`, `ciphertext`, mensagens.

**Amostragem:**
- Sucesso: 1% (ajustÃ¡vel)
- Erro: 100%
- Picos: 10% via flag

**Destino:**
- Tempo real: Loki (opcional) ou arquivo local NDJSON
- DiÃ¡rio: R2/MinIO â†’ `logs/yyyy/mm/dd/*.ndjson`
- RetenÃ§Ã£o: 30 dias

---

## ğŸ” Trilhas / Auditoria (JSONâœ¯Atomic)

**Forma canÃ´nica:**
```json
{
  "id": "...",
  "ts": "2026-01-03T...",
  "kind": "office.tool_call",
  "scope": {"tenant": "ubl"},
  "actor": {"email": "..."},
  "refs": {},
  "data": {"tool": "...", "args_min": {...}},
  "meta": {"service": "..."},
  "sig": null
}
```

**Kinds mÃ­nimos:**
- `office.tool_call`, `office.event`, `office.handover`
- `gateway.request_min`, `policy.eval_min`
- `auth.login_min`, `access.denied_min`
- `media.presign_min`, `webhook.delivery_min`

**Rollup diÃ¡rio:**
```bash
./infra/observability/jobs/rollup_trails_to_r2.sh [date]
# Uploads to: r2://ubl-audit/audit/YYYY/MM/trails_YYYY-MM-DD.ndjson
```

---

## âœ… Health Checklist

- [ ] Prometheus UI mostra todos os jobs "UP"
- [ ] OTEL Collector `/metrics` expÃµe sÃ©ries com prefixos corretos
- [ ] Dashboards renderizam sem "No data"
- [ ] Alertas configurados no Prometheus/Alertmanager
- [ ] Logs JSONL server-blind gravando (sem plaintext)
- [ ] Trilhas JSONâœ¯Atomic em R2/MinIO
- [ ] Rollup diÃ¡rio funcionando (cron/systemd timer)

---

## ğŸ”§ PrÃ³ximos Passos (P1)

- [ ] Assinatura Ed25519 de trilhas (tenant-opt-in)
- [ ] `/trace/:id` lookup por trace_id
- [ ] Amostragem dinÃ¢mica via flag (per-route/tool)
- [ ] Export VOD de trilhas para auditor externo (bundle .tar.gz)

---

## ğŸ“š ReferÃªncias

- **Blueprint 09** â€” Observabilidade & Auditoria
- **CONSTITUTION.md** â€” Normas de observabilidade server-blind
- **ErrorToken** â€” VocabulÃ¡rio fechado de erros
